{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    def __init__(self, lr, itr, regularization):\n",
    "        \"\"\"\n",
    "        learning_rate: A small value needed for gradient decent, default value id 0.1\n",
    "        iteration: Number of training iteration, default value is 10,000\n",
    "        \"\"\"\n",
    "        self.m = None # number of training samples\n",
    "        self.n = None # number of features\n",
    "        self.w = None # weights\n",
    "        self.b = None # bias\n",
    "        # will be the l1/l2 regularization class according to the regression model Lasso or ridge\n",
    "        self.regularization = regularization\n",
    "        self.lr = lr\n",
    "        self.itr = itr\n",
    "\n",
    "    def cost_function(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        y: Original target value\n",
    "        y_pred: predicted target value\n",
    "        \"\"\"\n",
    "        return (1 / (2*self.m)) * np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n",
    "\n",
    "    def hypothesis(self, w, b, X):\n",
    "        \"\"\"\n",
    "        weights: parameter value weight\n",
    "        X: Training samples\n",
    "        b: bias\n",
    "        \"\"\"\n",
    "        return np.dot(X, w)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        X: training data feature values ---> N Dimentional vector\n",
    "        y: training data target value -----> 1 Dimentional array\n",
    "        \"\"\"\n",
    "        # Insert constant ones for bias weights\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        # Target value should be in the shape of (n, 1)\n",
    "        # So, this will check that and change the shape to (n, 1), if not\n",
    "        try:\n",
    "            y.shape[1]\n",
    "        except IndexError as e:\n",
    "            # we need to change it to the 1 D array, not a list\n",
    "            print(\"ERROR: Target array should be a one dimentional array not a list\"\n",
    "                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n",
    "                  .format(shape_y_0=y.shape[0], shape_y=y.shape))\n",
    "            return\n",
    "\n",
    "        self.m = X.shape[0]\n",
    "\n",
    "        self.n = X.shape[1]\n",
    "\n",
    "        # initial weight\n",
    "        self.w = np.zeros((self.n, 1))\n",
    "\n",
    "        # bias\n",
    "        self.b = 0\n",
    "\n",
    "        for it in range(1, self.itr+1):\n",
    "            '''\n",
    "            # 1. Find the predicted value through the hypothesis\n",
    "            # 2. Find the Cost function value\n",
    "            # 3. Find the derivation of weights\n",
    "            # 4. Apply Gradient Decent\n",
    "            '''\n",
    "            y_pred = self.hypothesis(self.w, self.b, X)\n",
    "            # print(\"iteration\",it) # debugging\n",
    "            # print(\"y predict value\", y_pred)  # debugging\n",
    "            cost = self.cost_function(y, y_pred)\n",
    "            # print(\"Cost function\", cost)  # debugging\n",
    "            # find the derivative\n",
    "            dw = (1/self.m) * np.dot(X.T, (y_pred - y)) + \\\n",
    "                self.regularization.derivation(self.w)\n",
    "            # print(\"weights derivation\", dw)  # debugging\n",
    "            # db = -(2 / self.m) * np.sum((y_pred - y)) # bias derivation\n",
    "\n",
    "            # change the weight parameter.\n",
    "            self.w = self.w - self.lr * dw\n",
    "            # print(\"updated weights\",self.w) # debugging\n",
    "            # self.b = self.b - self.lr * db # change the bias parameter\n",
    "\n",
    "            # if it % 10 == 0: # print cost value every 10 iteration\n",
    "            #     print(\n",
    "            #         \"The Cost function for the iteration {}----->{} :)\".format(it, cost))\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        :param test_X: feature values to predict\n",
    "        \"\"\"\n",
    "        # Insert constant ones for bias weights\n",
    "        test_X = np.insert(test_X, 0, 1, axis=1)\n",
    "\n",
    "        y_pred = self.hypothesis(self.w, self.b, test_X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regularization class we want.\n",
    "class l1_regularization:\n",
    "    \"\"\"Regularization used for Lasson Regression\"\"\"\n",
    "    def __init__(self, lamda):\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.lamda * np.sum(np.abs(w))\n",
    "\n",
    "    def derivation(self, w):\n",
    "        \"Derivation of the regularization function\"\n",
    "        return self.lamda * np.sign(w)  # the sign function returns (-1 if x < 0, 0 if x==0, 1 if x > 0) taken from numpy documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression(Regression):\n",
    "    \"\"\"\n",
    "    Lasso Regression is one of the variance of the Linear Regression. This model doing the parameter learning\n",
    "    and regularization at the same time. This model uses the l1-regularization.\n",
    "    * Regularization will be one of the soluions to the Overfitting.\n",
    "    * Overfitting happens when the model has \"High Variance and low bias\". So, regularization adds a little bias to the model.\n",
    "    * This model will try to keep the balance between learning the parameters and the complexity of the model( tries to keep the parameter having small value and small degree of polynomial).\n",
    "    * The Regularization parameter(lamda) controls how severe  the regularization is.\n",
    "    * large lamda adds more bias , hence the Variance will go very small --> this may cause underfitting(Low bias and High Varinace).\n",
    "    * Lamda can be found by trial and error methos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lamda, lr, itr):\n",
    "        \"\"\"\n",
    "        lamda: Regularization factor.\n",
    "        learning_rate: A samll value needed for gradient decent, default value id 0.1\n",
    "        iteration: Number of training iteration, default value is 10,000\n",
    "        \"\"\"\n",
    "        self.regularization = l1_regularization(lamda)\n",
    "        super(LassoRegression, self).__init__(\n",
    "            lr, itr, self.regularization)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        X: training data feature values ---> N Dimentional vector\n",
    "        y: training data target value -----> 1 Dimentional array\n",
    "        \"\"\"\n",
    "        return super(LassoRegression, self).train(X, y)\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        test_X: Value need to be predicted\n",
    "        \"\"\"\n",
    "        return super(LassoRegression, self).predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing iris dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "y = y[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Number of training data samples-----> 150\n",
      "Number of training features --------> 4\n",
      "Shape of the target value ----------> (150, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
    "print(\"Number of training features --------> {}\".format(X.shape[1]))\n",
    "print(\"Shape of the target value ----------> {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.34766132  0.13326073  2.24400344  1.32072796  1.40849077  0.13016303\n",
      "  1.00169544  1.61486147  1.39655153  1.10310932  1.55790277  0.09679454\n",
      "  0.02267831  0.11749315  0.07173733  1.35568741  1.83643506  1.10999561\n",
      "  1.30117179  1.79458933  0.15546814  1.48554079  0.16406423  1.77865845\n",
      "  1.90325686  1.65882575  1.82661498  1.86120747  0.12865628  0.16597972\n",
      " -0.08056687  0.01921672  1.24763024  0.13309948  0.06049205  1.58748227\n",
      "  1.28605372  0.0882382   0.04606792  0.00654684  1.5999118   1.28185843\n",
      "  1.3598827   0.01114823  0.06542104  1.04125892  1.52298573  1.66587839\n",
      "  1.25903882  1.91164929]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Create the model\n",
    "model = LassoRegression(lamda=0.01, lr=0.01, itr=100)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.train(X_train, y_train)\n",
    "\n",
    "# Predict the value\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.24483889  0.03761411  2.2715246   1.33405184  1.33694778  0.05370715\n",
      "  1.02268739  1.82279194  1.34358547  1.05119935  1.69602325 -0.07898228\n",
      " -0.09769492 -0.05934532  0.00884577  1.41111792  1.97583097  1.0172658\n",
      "  1.25493292  1.93269844  0.01844701  1.5713074   0.09510153  1.88611888\n",
      "  1.96469601  1.85893351  1.79411754  2.03186589  0.01417685  0.01705282\n",
      " -0.14331182  0.0174873   1.22957329  0.0096596  -0.04785709  1.64845175\n",
      "  1.31122422 -0.03419918 -0.05490635 -0.09556517  1.68877592  1.36830938\n",
      "  1.35403276 -0.01053481 -0.04684519  0.9250893   1.48070026  1.713027\n",
      "  1.2358963   2.15165293]\n"
     ]
    }
   ],
   "source": [
    "# using lasso regression from sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=0.01, max_iter=100)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
